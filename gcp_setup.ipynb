{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup direnv & .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# MacOS\n",
    "brew install direnv\n",
    "\n",
    "# Ubuntu (Linux or Windows WSL2)\n",
    "sudo apt update\n",
    "sudo apt install -y direnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Create .env\n",
    "touch .env\n",
    "\n",
    "# Then add your environments variables in .env file\n",
    "'''\n",
    "# GCP Project\n",
    "GCP_PROJECT=<project_id_you_want>\n",
    "GCP_REGION=europe-west1\n",
    "\n",
    "# Cloud Storage\n",
    "BUCKET_NAME=nlp-shades-<your_github_name>\n",
    "\n",
    "# BigQuery\n",
    "BQ_REGION=EU\n",
    "BQ_DATASET=nlp_shades # must be letters (uppercase or lowercase), numbers, and underscores up to 1024 characters.\n",
    "DATA_SIZE=10k # 300k, 450k, all\n",
    "LOCAL_DATA_PATH=\"<path/to/the/package/model/dir>\"\n",
    "\n",
    "# Compute Engine\n",
    "INSTANCE=instance-nlp-shades-<your_github_name>\n",
    "\n",
    "# Docker\n",
    "GCR_IMAGE=nlp-shades-api\n",
    "GCR_REGION=eu.gcr.io\n",
    "'''\n",
    "\n",
    "# Create .envrc to load the environment variables\n",
    "touch .envrc\n",
    "echo dotenv  >> .envrc\n",
    "\n",
    "# Allow to load the .envrc\n",
    "direnv allow\n",
    "\n",
    "# ⚠️⚠️ If you update the .env file ⚠️⚠️\n",
    "direnv reload ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GCP Service account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Create a GCP account\n",
    "[here](https://github.com/lewagon/data-setup/blob/master/VM.md#google-cloud-platform-setup)\n",
    "\n",
    "2- Create a service account\n",
    "[here](https://github.com/lewagon/data-setup/blob/master/VM.md#google-cloud-cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Check your service account\n",
    "gcloud iam service-accounts list\n",
    "'''\n",
    "DISPLAY NAME                            EMAIL                                                              DISABLED\n",
    "Compute Engine default service account  161307270588-compute@developer.gserviceaccount.com                 False\n",
    "Compte de service (Wagon Bootcamp)      compte-de-service-wagon-bootca@PROJECT_ID.iam.gserviceaccount.com  False\n",
    "'''\n",
    "# ⚠️⚠️  If you are not on the right project ⚠️⚠️ \n",
    "gcloud config set project YOUR_PROJECT_ID\n",
    "\n",
    "# Then\n",
    "gcloud iam service-accounts list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Storage (create a bucket  to save the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Create a bucket (from variables in .env)\n",
    "gsutil mb -l $GCP_REGION -p $GCP_PROJECT gs://$BUCKET_NAME\n",
    "\n",
    "# Then set gcloud on the correct GCP project ID\n",
    "gcloud config set project $GCP_PROJECT\n",
    "\n",
    "# And check \n",
    "gsutil ls\n",
    "'''--> gs://YOUR BUCKET NAME/'''\n",
    "\n",
    "# You can also check from the Google Cloud Console with the Cloud Storage module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Query (store the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset where we’ll store & query preprocessed data !\n",
    "bq mk --project_id $GCP_PROJECT --data_location $BQ_REGION -d $BQ_DATASET\n",
    "\n",
    "# Then check par acquis de conscience\n",
    "bq show $BQ_DATASET\n",
    "\n",
    "# Finally create N new tables you want for train\n",
    "bq mk --location=$GCP_REGION $BQ_DATASET.raw_10k # small test\n",
    "bq mk --location=$GCP_REGION $BQ_DATASET.raw_300k  # 50% of the dataset\n",
    "bq mk --location=$GCP_REGION $BQ_DATASET.raw_450k  # 75% of the dataset\n",
    "bq mk --location=$GCP_REGION $BQ_DATASET.raw_all # all\n",
    "\n",
    "# Then :)\n",
    "bq show $BQ_DATASET.raw_10k\n",
    "bq show $BQ_DATASET.raw_300k\n",
    "bq show $BQ_DATASET.raw_450k\n",
    "bq show $BQ_DATASET.raw_all\n",
    "\n",
    "# You can also check from the Google Cloud Console with the BigQuerry module\n",
    "\n",
    "# 1- First load the data on Big Query (cf. data.py)\n",
    "make load_10k_data_to_bq\n",
    "make load_300k_data_to_bq\n",
    "make load_450k_data_to_bq\n",
    "make load_all_data_to_bq\n",
    "\n",
    "# 1bis- Go on Big Query to check the tables\n",
    "\n",
    "# 2- Run locally the model you want with the sample you want (cf. main.py) with either the csv file or by querying Big Query\n",
    "make run_ner_on_10k  # or\n",
    "make run_ner_on_300k # or\n",
    "make run_ner_on_450k # or\n",
    "make run_ner_on_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Virtual Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- GO \n",
    "[HERE](https://github.com/lewagon/data-setup/blob/master/VM.md#virtual-machine-vm)\n",
    "\n",
    "2- If you know exactly what type of VM you want to create, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "INSTANCE=project-instance\n",
    "IMAGE_PROJECT=ubuntu-os-cloud\n",
    "IMAGE_FAMILY=ubuntu-2204-lts\n",
    "\n",
    "gcloud compute instances create $INSTANCE --image-project=$IMAGE_PROJECT --image-family=$IMAGE_FAMILY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup The Virtual Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Install zsh and omz \n",
    "sudo apt update\n",
    "sudo apt install -y zsh\n",
    "sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n",
    "\n",
    "# Install pyenv and pyenv-virtualenv\n",
    "git clone https://github.com/pyenv/pyenv.git ~/.pyenv\n",
    "git clone https://github.com/pyenv/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv\n",
    "\n",
    "# Open ~/.zshrc in a Terminal code editor\n",
    "nano ~/.zshrc\n",
    "'''\n",
    "Add pyenv, ssh-agent and direnv to the list of zsh plugins on the line with plugins=(git) in ~/.zshrc: in the end, you should have plugins=(git pyenv ssh-agent direnv). Then, exit and save (Ctrl + X, Y, Enter).\n",
    "'''\n",
    "\n",
    "# Make sure that the modifications were indeed saved\n",
    "cat ~/.zshrc | grep \"plugins=\"\n",
    "\n",
    "# Add the pyenv initialization script to your ~/.zprofile\n",
    "cat << EOF >> ~/.zprofile\n",
    "export PYENV_ROOT=\"\\$HOME/.pyenv\"\n",
    "export PATH=\"\\$PYENV_ROOT/bin:\\$PATH\"\n",
    "eval \"\\$(pyenv init --path)\"\n",
    "EOF\n",
    "\n",
    "# Install Python\n",
    "sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \\\n",
    "libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\n",
    "libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev \\\n",
    "python3-dev\n",
    "\n",
    "# Now we need to start a new user session so that the updates in ~/.zshrc and ~/.zprofile are taken into account\n",
    "zsh --login\n",
    "\n",
    "# Install the same python version that you use for the project\n",
    "pyenv install 3.10.6\n",
    "pyenv global 3.10.6\n",
    "pyenv virtualenv 3.10.6 nlp-shades # ⚠️ don't forget to update it\n",
    "pyenv global nlp-shades # ⚠️ don't forget to update it\n",
    "\n",
    "# Github auth\n",
    "# ⚠️ Run this single command on YOUR machine, NOT in the VM ⚠️\n",
    "gcloud compute scp ~/.ssh/id_ed25519 $USER@$INSTANCE:~/.ssh/\n",
    "'''\n",
    "Check that $USER in your machine is the same that in your VM, it may be different ...\n",
    "If not, replace it with the $USER displayed on your VM !!\n",
    "\n",
    "gcloud compute scp ~/.ssh/id_ed25519 <user_from_VM>@$INSTANCE:~/.ssh/\n",
    "'''\n",
    "\n",
    "# ⚠️ Then, resume running commands in the VM ⚠️\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_ed25519\n",
    "'''\n",
    "It should display:\n",
    "\n",
    "Agent pid 18827\n",
    "Identity added: /home/<user_from_VM>/.ssh/id_ed25519 (<your_email>)\n",
    "'''\n",
    "\n",
    "# Python code authentication to GCP\n",
    "'''\n",
    "The code of your package needs to be able to access your Big Query data warehouse.\n",
    "To do so, we will login to your account using the command below\n",
    "'''\n",
    "gcloud auth application-default login\n",
    "'''\n",
    "It should display a link to authenticate to GCP, copy/paste the link in your browser and authorize the acces. Then copy/paste the authorization code in the VM console.\n",
    "'''\n",
    "\n",
    "# Let’s verify that your Python code can now access your GCP resources. First, install some packages\n",
    "pip install -U pip\n",
    "pip install google-cloud-storage\n",
    "\n",
    "# Then, run Python code from the CLI. This should list your GCP buckets:\n",
    "python -c \"from google.cloud import storage; \\\n",
    "    buckets = storage.Client().list_buckets(); \\\n",
    "    [print(b.name) for b in buckets]\"\n",
    "\n",
    "# Let’s run a few tests inside your VM Terminal before we install it\n",
    "'''\n",
    "Default shell is /usr/bin/zsh\n",
    "'''\n",
    "echo $SHELL\n",
    "'''\n",
    "Python version is [version_installed]\n",
    "'''\n",
    "python --version\n",
    "'''\n",
    "Active GCP project is the same as $GCP_PROJECT in your .env file\n",
    "'''\n",
    "gcloud config list project\n",
    "\n",
    "\n",
    "# Your VM is now a data science beast 🔥 :) :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train in the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First, you have to clone your package, install its requirements\n",
    "'''\n",
    "\n",
    "# You can copy your code to the VM by cloning your GitHub project\n",
    "git clone git@github.com:ekmillenium/nlp_shades_of_movie_reviews.git # ⚠️ don't forget to update it with the right project\n",
    "\n",
    "# Enter the directory\n",
    "cd <path/to/the/package/model/dir>\n",
    "\n",
    "# Create a .env file with all required parameters to use your package\n",
    "touch .env\n",
    "\n",
    "# Then updte the .env file with your parameters\n",
    "cat > .env \n",
    "'''# GCP Project'''\n",
    "GCP_PROJECT=<project_id_you_want>\n",
    "GCP_REGION=europe-west1\n",
    "'''# Cloud Storage'''\n",
    "BUCKET_NAME=nlp-shades-<your_github_name>\n",
    "'''# BigQuery'''\n",
    "BQ_REGION=EU\n",
    "BQ_DATASET=nlp_shades \n",
    "DATA_SIZE=10k\n",
    "LOCAL_DATA_PATH=\"<path/to/the/package/model/dir>\"\n",
    "'''# Compute Engine'''\n",
    "INSTANCE=instance-nlp-shades-<your_github_name>\n",
    "'''# Docker'''\n",
    "GCR_IMAGE=nlp-shades-api\n",
    "\n",
    "# Install direnv to load your .env\n",
    "sudo apt update\n",
    "sudo apt install -y direnv\n",
    "\n",
    "# Reconnect (simulate a user reboot) so that direnv works\n",
    "zsh --login\n",
    "\n",
    "# Allow your .envrc\n",
    "direnv allow .\n",
    "\n",
    "# Install the project package\n",
    "pip install .\n",
    "\n",
    "# Finally have fun !! :) :)\n",
    "make <the_command_you_want>\n",
    "\n",
    "## ⚠️⚠️ Switch OFF your VM to finish ⚠️⚠️ ##\n",
    "gcloud compute instances stop $INSTANCE\n",
    "gcloud compute instances list\n",
    "gcloud compute instances start $INSTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# First, You need to have the Docker daemon running on your machine to be able to build and run the image.\n",
    "--> Launch the Docker app\n",
    "\n",
    "# Then, create a Dockerfile at the root\n",
    "touch Dockerfile\n",
    "\n",
    "# Update the Dockerfile\n",
    "'''\n",
    "FROM python:3.10.6-bullseye\n",
    "\n",
    "# Copy packages\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install packages\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy Makefile\n",
    "COPY Makefile Makefile\n",
    "\n",
    "# Download Spacy pipeline\n",
    "RUN make download_spacy_pipeline\n",
    "\n",
    "# Copy app directories and files\n",
    "COPY interface /interface\n",
    "COPY ml_logic /ml_logic\n",
    "#COPY scraper /scraper\n",
    "COPY setup.py setup.py\n",
    "COPY params.py params.py\n",
    "RUN pip install .\n",
    "\n",
    "# Run api server\n",
    "CMD uvicorn interface.api:app --host 0.0.0.0 --port 8000\n",
    "'''\n",
    "\n",
    "# Build the API image\n",
    "docker build --tag=$GCR_IMAGE:dev .\n",
    "\n",
    "# The image should be visible with the following command\n",
    "docker images\n",
    "\n",
    "# Check the API image\n",
    "docker run -it -e PORT=8000 -p 8000:8000 $GCR_IMAGE:dev sh\n",
    "\n",
    "# Run the API image\n",
    "docker run -e PORT=8000 -p 8000:8000 --env-file .env $GCR_IMAGE:dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the API (Google Container Registry)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s make sure to enable the [Google Container Registry API](https://console.cloud.google.com/apis/enableflow?apiid=containerregistry.googleapis.com&redirect=https:%2F%2Fcloud.google.com%2Fcontainer-registry%2Fdocs%2Fquickstart&authuser=1&project=wagon-nlp-project) for your project in GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Once this is done, let’s allow the docker command to push an image to GCP\n",
    "gcloud auth configure-docker\n",
    "\n",
    "# Build and push the Image on GCR\n",
    "docker build -t $GCR_REGION/$GCP_PROJECT/$GCR_IMAGE:dev .\n",
    "\n",
    "# Test if the image run correctly\n",
    "docker run -e PORT=8000 -p 8000:8000 --env-file .env $GCR_REGION/$GCP_PROJECT/$GCR_IMAGE:dev\n",
    "\n",
    "# Now, ladies and gentlemen ! Push the image on Google Container Registry\n",
    "docker push $GCR_REGION/$GCP_PROJECT/$GCR_IMAGE:dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image should be visible in the [GCP](https://console.cloud.google.com/gcr/) console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Provide environment variables to the container\n",
    "gcloud run deploy --env-vars-file .env.yaml\n",
    "\n",
    "# Then create a .env.yaml file\n",
    "touch .env.yaml\n",
    "\n",
    "# Update .env.yaml file\n",
    "'''\n",
    "# GCP Project\n",
    "GCP_PROJECT: \"<project_id_you_want>\"\n",
    "GCP_REGION: \"europe-west1\"\n",
    "\n",
    "# Cloud Storage\n",
    "BUCKET_NAME: \"nlp-shades-<you_github_name>\"\n",
    "\n",
    "# BigQuery\n",
    "BQ_REGION: \"EU\"\n",
    "BQ_DATASET: \"nlp_shades\" # must be letters (uppercase or lowercase), numbers, and underscores up to 1024 characters.\n",
    "DATA_SIZE: \"10k\" # 300k, 450k, all\n",
    "LOCAL_DATA_PATH: \"<path/to/the/package/model/dir>\"\n",
    "\n",
    "# Compute Engine\n",
    "INSTANCE: \"instance-nlp-shades-<you_github_name>\"\n",
    "\n",
    "# Docker\n",
    "GCR_IMAGE: \"nlp-shades-api\"\n",
    "GCR_REGION: \"eu.gcr.io\"\n",
    "'''\n",
    "\n",
    "# Finally run the following command\n",
    "gcloud run deploy --image $GCR_REGION/$GCP_PROJECT/$GCR_IMAGE:dev --memory $MEMORY --region $GCP_REGION --env-vars-file .env.yaml\n",
    "\n",
    "# After confirmation, you should see something like this, indicating that the service is live 🎉\n",
    "'''\n",
    "Service name (wagon-data-tpl-image):\n",
    "Allow unauthenticated invocations to [wagon-data-tpl-image] (y/N)?  y\n",
    "\n",
    "Deploying container to Cloud Run service [nlp-shades-api] in project [wagon-nlp-project] region [europe-west1]\n",
    "✓ Deploying new service... Done.\n",
    "  ✓ Creating Revision... Revision deployment finished. Waiting for health check to begin.\n",
    "  ✓ Routing traffic...\n",
    "  ✓ Setting IAM Policy...\n",
    "Done.\n",
    "Service [wagon-data-tpl-image] revision [wagon-data-tpl-image-00001-kup] has been deployed and is serving 100 percent of traffic.\n",
    "Service URL: https://wagon-data-tpl-image-xi54eseqrq-ew.a.run.app\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💸 Stop everything and save money 💸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the current instance\n",
    "gcloud run services stop $INSTANCE\n",
    "\n",
    "# Stop local docker image too\n",
    "docker ps # copy/paste the CONTAINER ID\n",
    "docker stop [CONTAINER ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PENSEZ À AJOUTER MANUELLEMENT CHECKPOINT_FILE DANS LA VM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_shades",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
